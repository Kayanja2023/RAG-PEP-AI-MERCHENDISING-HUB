# RAG Chatbot - Production-Grade Document QA System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.22+-red.svg)](https://streamlit.io/)
[![LangChain](https://img.shields.io/badge/langchain-1.0+-green.svg)](https://www.langchain.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A production-ready Retrieval-Augmented Generation (RAG) chatbot leveraging OpenAI's GPT-4, FAISS vector search, and LangChain's orchestration framework. Designed for enterprise document management with robust error handling, atomic file operations, and persistent conversation memory.

---

## Table of Contents

- [Overview](#overview)
- [System Architecture](#system-architecture)
- [Quick Start](#quick-start)
- [Deployment](#deployment)
- [Project Structure](#project-structure)
- [Configuration Options](#configuration-options)
- [Key Features & Implementation](#key-features--implementation)
- [RAG Pipeline Deep Dive](#rag-pipeline-deep-dive)
- [Development & Testing](#development--testing)
- [Troubleshooting](#troubleshooting)
- [Performance Characteristics](#performance-characteristics)
- [Security Considerations](#security-considerations)
- [Advanced Usage](#advanced-usage)
- [Technical Stack](#technical-stack)
- [Contributing](#contributing)
- [License](#license)

---

## Overview

This application implements a sophisticated RAG pipeline that enables users to:
- Upload and index documents in multiple formats (TXT, PDF, DOCX, Markdown)
- Query document content using natural language
- Maintain contextual conversation history across sessions
- Manage document lifecycle with atomic operations and duplicate prevention

### Key Differentiators
- **Atomic File Operations**: Prevents data corruption during concurrent uploads
- **Smart Duplicate Detection**: Content-hash based file signature tracking
- **Persistent Chat Memory**: Conversation history survives document deletions
- **Zero-Downtime Updates**: Generation-based UI state management prevents infinite loops
- **Graceful Degradation**: Chat interface remains accessible even without documents

---

## ğŸ—ï¸ System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Streamlit Frontend                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Upload Widget  â”‚â”€â”€â”€â–¶â”‚ Chat Interfaceâ”‚â—€â”€â”€â”€â”‚ Document Managerâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                 â”‚
                    â–¼                                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   RAG Engine Layer     â”‚       â”‚   Config Layer     â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ Document Loader  â”‚  â”‚       â”‚  â”‚ File Manager â”‚  â”‚
        â”‚  â”‚ Text Chunker     â”‚  â”‚       â”‚  â”‚ Validator    â”‚  â”‚
        â”‚  â”‚ Vector Store     â”‚  â”‚       â”‚  â”‚ Atomic Write â”‚  â”‚
        â”‚  â”‚ LLM Chain        â”‚  â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   External Services    â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ OpenAI API       â”‚  â”‚
        â”‚  â”‚ - Embeddings     â”‚  â”‚
        â”‚  â”‚ - Chat LLM       â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
Upload â†’ Validation â†’ Signature â†’ Duplicate Check â†’ Atomic Write
                                                          â”‚
                                                          â–¼
                                            Document Storage (data/documents/)
                                                          â”‚
                                                          â–¼
                                            Document Loader (Multi-format)
                                                          â”‚
                                                          â–¼
                                            Text Splitter (Chunking)
                                                          â”‚
                                                          â–¼
                                            Embeddings (OpenAI text-embedding-3-small)
                                                          â”‚
                                                          â–¼
                                            Vector Store (FAISS Index)
                                                          â”‚
User Query â†’ Retriever â†’ Context Injection â†’ LLM â†’ Response
              â”‚                               â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   (Top-K Similar Chunks)
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python**: 3.8 or higher
- **OpenAI API Key**: Required for embeddings and chat completion
- **Virtual Environment**: Recommended for dependency isolation

### Installation

```powershell
# Clone the repository
git clone <repository-url>
cd Rag

# Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=sk-your-openai-api-key-here
```

### Running the Application

```powershell
streamlit run app.py
```

Access the application at: `http://localhost:8501`

---

## Deployment

### Deploying to Streamlit Cloud

This application is designed for easy deployment to Streamlit Cloud (formerly Streamlit Sharing).

#### Step 1: Push to GitHub

Ensure your code is pushed to a GitHub repository:

```powershell
git add .
git commit -m "Prepare for deployment"
git push origin main
```

#### Step 2: Deploy on Streamlit Cloud

1. Go to [share.streamlit.io](https://share.streamlit.io)
2. Sign in with your GitHub account
3. Click "New app"
4. Select your repository: `Kayanja2023/Rag`
5. Set main file path: `app.py`
6. Set branch: `main`

#### Step 3: Configure Secrets

**CRITICAL**: You must configure the OpenAI API key as a secret in Streamlit Cloud:

1. In your app dashboard, click "Settings" (âš™ï¸)
2. Navigate to "Secrets" section
3. Add your secrets in TOML format:

```toml
OPENAI_API_KEY = "sk-your-actual-openai-api-key-here"
```

4. Click "Save"

#### Step 4: Deploy

Click "Deploy" and wait for the application to start. The deployment process will:
- Clone your repository
- Install dependencies from `requirements.txt`
- Start the Streamlit application

**Note**: The initial deployment may take 2-5 minutes as it installs all dependencies.

### Deployment Checklist

Before deploying, ensure:

- [ ] Repository is public or Streamlit Cloud has access
- [ ] `requirements.txt` is present and up-to-date
- [ ] `OPENAI_API_KEY` is configured in Streamlit Cloud secrets
- [ ] `.env` file is listed in `.gitignore` (never commit API keys!)
- [ ] `data/` directory is in `.gitignore` (documents are user-specific)
- [ ] Code is tested locally before deployment

### Troubleshooting Deployment

#### âŒ Error: `OpenAIError: The api_key client option must be set`

**This is the most common deployment error!**

**Root Cause**: The OpenAI API key is not configured in Streamlit Cloud secrets.

**Solution**: 
1. Go to [share.streamlit.io](https://share.streamlit.io)
2. Click your app â†’ **Settings** (âš™ï¸) â†’ **Secrets**
3. Add in TOML format:
   ```toml
   OPENAI_API_KEY = "sk-your-actual-key-here"
   ```
4. Click **Save** and wait for automatic reboot (~30 seconds)

**Verification**: Check the logs - you should see the app start successfully without the OpenAI error.

**Error**: `ModuleNotFoundError: No module named 'X'`

**Solution**: Ensure the missing package is in `requirements.txt`

**Error**: App crashes after document upload

**Solution**: Streamlit Cloud has limited storage. Documents are ephemeral and will be lost on reboot.

### Environment-Specific Configuration

For production deployments, consider:

```python
# config.py - Add environment detection
import os

# Use temp directories in cloud environments
if os.environ.get("STREAMLIT_RUNTIME_ENV") == "cloud":
    DOCS_DIR = "/tmp/documents"
    FAISS_DIR = "/tmp/faiss_store"
else:
    DOCS_DIR = os.path.join(os.path.dirname(__file__), "data", "documents")
    FAISS_DIR = os.path.join(os.path.dirname(__file__), "data", "faiss_store")
```

### Monitoring Deployed Application

- **Logs**: View real-time logs in Streamlit Cloud dashboard
- **Status**: Check app status and resource usage
- **Analytics**: Monitor user interactions (requires Streamlit for Teams)

---

## ğŸ“ Project Structure

```
rag-chatbot/
â”‚
â”œâ”€â”€ app.py                      # Main Streamlit application (UI & orchestration)
â”œâ”€â”€ rag_engine.py              # RAG pipeline (embeddings, retrieval, LLM chain)
â”œâ”€â”€ config.py                  # Configuration & file management utilities
â”œâ”€â”€ utils.py                   # Document text extraction utilities
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env                       # Environment variables (create manually)
â”œâ”€â”€ .env.example              # Environment template
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/            # Uploaded document storage (git-ignored)
â”‚   â””â”€â”€ faiss_store/          # FAISS vector index (git-ignored)
â”‚
â””â”€â”€ __pycache__/              # Python bytecode cache
```

### Module Responsibilities

#### `app.py` - Application Layer
- **Streamlit UI**: Chat interface, file uploader, document list
- **Session State Management**: Upload tracking, conversation history
- **Upload Orchestration**: Signature generation, duplicate detection, atomic processing
- **Error Handling**: User-facing error messages and recovery

#### `rag_engine.py` - Core RAG Pipeline
- **Document Loading**: Multi-format loader (TXT, PDF, DOCX, Markdown)
- **Text Chunking**: Recursive character text splitter with overlap
- **Embeddings**: OpenAI text-embedding-3-small (cached)
- **Vector Store**: FAISS index creation and management (cached)
- **LLM Chain**: Conversational retrieval chain with memory

#### `config.py` - Configuration & Utilities
- **Path Configuration**: Document and FAISS directories
- **RAG Parameters**: Chunk size, model selection, temperature
- **File Management**: Validation, atomic writes, unique naming
- **Document Operations**: CRUD operations with error handling

#### `utils.py` - Document Processing
- **Text Extraction**: Format-specific parsers (PDF, DOCX, TXT, MD)
- **Error Handling**: Graceful fallbacks for corrupted files

---

## âš™ï¸ Configuration Options

### Environment Variables (`.env`)

| Variable | Required | Description | Example |
|----------|----------|-------------|---------|
| `OPENAI_API_KEY` | âœ… Yes | OpenAI API authentication key | `sk-proj-...` |

### Application Configuration (`config.py`)

#### RAG Pipeline Parameters

```python
# Text Chunking
CHUNK_SIZE = 1000          # Characters per chunk (balance: context vs. precision)
CHUNK_OVERLAP = 200        # Overlap between chunks (preserves semantic continuity)

# LLM Configuration
MODEL = "gpt-4"            # Options: gpt-4, gpt-4-turbo, gpt-3.5-turbo
TEMPERATURE = 0.7          # Response creativity (0.0=deterministic, 1.0=creative)

# Retrieval Parameters
SEARCH_K = 3               # Number of relevant chunks to retrieve per query
```

#### File Constraints

```python
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB per file
ALLOWED_EXTENSIONS = ["txt", "pdf", "docx", "md"]
```

### Performance Tuning Guidelines

| Use Case | Chunk Size | Search K | Temperature | Model |
|----------|------------|----------|-------------|-------|
| Technical Docs | 1500 | 5 | 0.3 | gpt-4 |
| Legal Documents | 2000 | 7 | 0.2 | gpt-4 |
| General Knowledge | 1000 | 3 | 0.7 | gpt-3.5-turbo |
| Creative Content | 800 | 4 | 0.9 | gpt-4 |

---

## ğŸ”§ Key Features & Implementation Details

### 1. Atomic File Uploads

**Problem**: Concurrent uploads or crashes can corrupt files.

**Solution**: Atomic write-then-rename pattern
```python
# Write to temporary file
with open(tmp_path, "wb") as f:
    f.write(data)

# Atomic rename (OS-level operation)
tmp_path.replace(dest_path)  # Works on Windows & POSIX
```

**Benefits**:
- No partial writes
- Crash-safe operations
- Thread-safe file creation

### 2. Smart Duplicate Detection

**Problem**: Re-uploading identical files wastes resources.

**Solution**: Content-hash based signatures
```python
signature = (filename, size, mime_type, content_hash[:8])
```

**Features**:
- Detects identical files with different names
- Prevents vector store redundancy
- Session-persistent tracking (last 50 uploads)

### 3. Generation-Based UI State Management

**Problem**: Streamlit re-runs cause infinite upload loops.

**Solution**: Generation counters reset widget state
```python
key=f"uploader_{st.session_state.upload_generation}"
st.session_state.upload_generation += 1  # Reset after save
```

**Benefits**:
- Clean UI after successful uploads
- Prevents stale file handles
- User-friendly "auto-clear" behavior

### 4. Persistent Chat History

**Problem**: Chat history disappears on document deletion or page refresh.

**Solution**: Backup-restore pattern + decoupled state
```python
# Backup before destructive operations
messages_backup = st.session_state.messages.copy()
session_histories_backup = st.session_state.session_histories.copy()

# Perform operation
st.cache_resource.clear()

# Restore chat history
st.session_state.messages = messages_backup
st.session_state.session_histories = session_histories_backup
```

### 5. Graceful Degradation

**Problem**: App becomes unusable when no documents exist.

**Solution**: Conditional UI rendering
- Chat history always visible (even without documents)
- Input disabled with helpful message
- Re-enables automatically when documents uploaded

---

## ğŸ” RAG Pipeline Deep Dive

### Document Ingestion Pipeline

```python
# 1. Document Loading (format-aware)
TextLoader â†’ PyPDFLoader â†’ Docx2txtLoader
              â†“
         Raw Documents

# 2. Text Chunking (semantic preservation)
RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,  # Prevents context loss at boundaries
    separators=["\n\n", "\n", ". ", " ", ""]
)
              â†“
         Text Chunks (with metadata)

# 3. Embedding Generation (cached)
OpenAI text-embedding-3-small (1536 dimensions)
              â†“
         Vector Embeddings

# 4. Index Creation (optimized for similarity search)
FAISS Index (L2 distance metric)
              â†“
         Persistent Vector Store
```

### Query Processing Pipeline

```python
# 1. User Query
"What are the main conclusions in the report?"
              â†“
# 2. Query Embedding
OpenAI text-embedding-3-small
              â†“
# 3. Similarity Search
FAISS retriever (top-k=3 chunks)
              â†“
# 4. Context Assembly
format_docs(retrieved_chunks) â†’ Combined context string
              â†“
# 5. Prompt Construction
ChatPromptTemplate([
    system_message + context,
    conversation_history,
    user_query
])
              â†“
# 6. LLM Invocation
GPT-4 (with conversation memory)
              â†“
# 7. Response Streaming
StrOutputParser â†’ User-facing response
```

### Conversation Memory Architecture

```python
# Session-based memory (isolated per user)
RunnableWithMessageHistory(
    chain=rag_chain,
    get_session_history=lambda sid: session_histories[sid],
    input_messages_key="input",
    history_messages_key="chat_history"
)
```

**Memory Persistence**:
- Stored in `st.session_state.session_histories`
- Survives document operations (backup-restore)
- Cleared only on explicit user action

---

## ğŸ› ï¸ Development & Testing

### Running in Development Mode

```powershell
# Enable hot reload
streamlit run app.py --server.runOnSave true

# Custom port
streamlit run app.py --server.port 8502
```

### Manual Testing Checklist

- [ ] Upload single file (each format: TXT, PDF, DOCX, MD)
- [ ] Upload multiple files simultaneously
- [ ] Upload duplicate file (should auto-rename)
- [ ] Delete document (verify chat history persists)
- [ ] Delete all documents (verify input disabled, history visible)
- [ ] Ask question and verify relevant response
- [ ] Test conversation memory (multi-turn dialogue)
- [ ] Clear chat history (verify clean slate)

### Common Edge Cases

1. **Large File Upload (>50MB)**: Should show validation error
2. **Corrupted PDF**: Graceful error with fallback parsing
3. **Empty Document**: Should process but may produce poor retrieval
4. **Special Characters in Filename**: Handled by unique naming
5. **Concurrent Uploads**: Atomic writes prevent corruption

---

## ğŸš¨ Troubleshooting

### Issue: FAISS Dimension Mismatch

**Symptom**: `AssertionError: Index dimension mismatch`

**Cause**: Embedding model changed after index creation

**Solution**:
```powershell
# Delete existing index
Remove-Item -Path "data\faiss_store\*" -Force -Recurse

# Restart app (will rebuild index automatically)
streamlit run app.py
```

### Issue: OpenAI API Rate Limit

**Symptom**: `RateLimitError: You exceeded your current quota`

**Solutions**:
1. Check API key billing status
2. Reduce `SEARCH_K` (fewer chunks = fewer tokens)
3. Switch to `gpt-3.5-turbo` (cheaper model)
4. Add exponential backoff retry logic

### Issue: Slow Document Processing

**Symptom**: Long wait times during upload

**Diagnosis**:
```python
# Add timing diagnostics
import time
start = time.time()
chunks = splitter.split_documents(docs)
print(f"Chunking took {time.time() - start:.2f}s")
```

**Solutions**:
- Reduce `CHUNK_SIZE` for faster processing
- Process documents asynchronously (advanced)
- Use smaller embedding model (trade-off: accuracy)

### Issue: Memory Leak (Long-Running Session)

**Symptom**: Streamlit becomes slow over time

**Solution**:
```python
# Periodic cleanup in config.py (already implemented)
if len(st.session_state.processed_uploads) > 50:
    st.session_state.processed_uploads = dict(sorted_items[-50:])
```

---

## ğŸ“Š Performance Characteristics

### Scalability Limits

| Metric | Recommended | Maximum | Notes |
|--------|-------------|---------|-------|
| Documents | 100-500 | 1000 | FAISS performs well in-memory |
| Total Size | <500MB | 2GB | Limited by server RAM |
| Chunk Count | 5k-10k | 50k | Retrieval latency increases linearly |
| Concurrent Users | 1 | 5 | Streamlit single-threaded by default |

### Latency Benchmarks

| Operation | Typical | Fast | Slow |
|-----------|---------|------|------|
| Single File Upload | 2s | 1s | 5s |
| Index Rebuild (100 docs) | 15s | 10s | 30s |
| Query Response | 3s | 2s | 8s |
| Document Deletion | <1s | 0.5s | 2s |

*Tested on: Intel i7, 16GB RAM, 50Mbps connection*

---

## ğŸ” Security Considerations

### Current Implementation

âœ… **Secure**:
- API keys in environment variables (not hardcoded)
- File type validation (prevents arbitrary code execution)
- Atomic writes (prevents race conditions)

âš ï¸ **Considerations for Production**:
- No authentication/authorization (anyone can access)
- No encryption at rest (documents stored plaintext)
- No rate limiting (vulnerable to abuse)
- OpenAI API key exposed to server (not user-isolated)

### Production Hardening Recommendations

```python
# 1. Add user authentication
import streamlit_authenticator as stauth

# 2. Encrypt documents at rest
from cryptography.fernet import Fernet

# 3. Add rate limiting
from slowapi import Limiter

# 4. Implement user-specific storage
DOCS_DIR = f"data/documents/{user_id}/"

# 5. Sanitize file inputs
import bleach
filename = bleach.clean(uploaded_file.name)
```

---

## ğŸ§ª Advanced Usage

### Custom Prompt Engineering

Edit the system prompt in `rag_engine.py`:

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a domain-specific expert assistant.

Guidelines:
- Cite source documents when providing answers
- Indicate confidence level (high/medium/low)
- Suggest follow-up questions for clarification

Context: {context}
Documents: {document_list}
"""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])
```

### Multi-Index Support

```python
# Create separate indexes per document type
def load_vector_store(doc_type="all"):
    if doc_type == "all":
        return FAISS.load_local(FAISS_DIR, embeddings)
    else:
        return FAISS.load_local(f"{FAISS_DIR}_{doc_type}", embeddings)
```

### Hybrid Search (Keyword + Semantic)

```python
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import BM25Retriever

# Combine sparse (BM25) and dense (FAISS) retrievers
bm25_retriever = BM25Retriever.from_documents(docs)
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever],
    weights=[0.3, 0.7]
)
```

---

## ğŸ“š Technical Stack

### Core Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `streamlit` | 1.22+ | Web UI framework |
| `langchain` | 1.0+ | RAG orchestration |
| `langchain-openai` | 1.0+ | OpenAI integrations |
| `faiss-cpu` | 1.7+ | Vector similarity search |
| `pdfplumber` | 0.7+ | PDF text extraction |
| `python-docx` | 0.8+ | DOCX parsing |
| `python-dotenv` | 1.0+ | Environment management |

### Architecture Patterns

- **Caching Strategy**: `@st.cache_resource` for embeddings & vector store
- **State Management**: Streamlit session state for UI persistence
- **Error Handling**: Try-except with user-friendly messages
- **File I/O**: Atomic operations with temp files
- **Memory Management**: LRU-style tracking with size limits

---

## ğŸ¤ Contributing

### Development Setup

```powershell
# Install dev dependencies
pip install -r requirements.txt

# Run linting
flake8 app.py rag_engine.py config.py utils.py

# Format code
black app.py rag_engine.py config.py utils.py
```

### Code Style Guidelines

- **PEP 8** compliance for Python code
- **Docstrings** for all public functions
- **Type hints** for function signatures (when possible)
- **Error handling** with specific exception types

---

## ğŸ“ License

This project is licensed under the MIT License. See `LICENSE` file for details.

---

## ğŸ™ Acknowledgments

- **LangChain**: Excellent RAG framework and abstractions
- **Streamlit**: Rapid prototyping for data applications
- **OpenAI**: State-of-the-art embeddings and language models
- **Facebook AI (Meta)**: FAISS vector search library

---

## ğŸ“§ Support

For issues, questions, or feature requests:
1. Check the **Troubleshooting** section above
2. Review closed issues on GitHub
3. Open a new issue with reproduction steps

---

**Built with â¤ï¸ for intelligent document interaction**
